diff -urN b/runtime/hsa-runtime/core/inc/amd_gpu_agent.h a/runtime/hsa-runtime/core/inc/amd_gpu_agent.h
--- b/runtime/hsa-runtime/core/inc/amd_gpu_agent.h	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/inc/amd_gpu_agent.h	2025-11-23 18:09:43.847233830 +0800
@@ -439,9 +439,21 @@
   /// @brief Force a WC flush on PCIe devices by doing a write and then read-back
   __forceinline void PcieWcFlush(void *ptr, size_t size) const {
     if (!xgmi_cpu_gpu_) {
+#if defined(_M_X64)
       _mm_sfence();
+#elif defined(__loongarch_lp64)
+      asm("dbar 0");
+#elif defined(__riscv)
+      asm volatile("fence w, w");
+#endif
       *((uint8_t*)ptr + size - 1) = *((uint8_t*)ptr + size - 1);
+#if defined(_M_X64)
       _mm_mfence();
+#elif defined(__loongarch_lp64)
+      asm("dbar 0");
+#elif defined(__riscv)
+      asm volatile("fence iorw, iorw");
+#endif
       auto readback = *(reinterpret_cast<volatile uint8_t*>(ptr) + size - 1);
       UNUSED(readback);
     }
diff -urN b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp
--- b/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/runtime/amd_aql_queue.cpp	2025-11-23 18:23:03.688417750 +0800
@@ -471,7 +471,13 @@
     HSAKMT_CALL(hsaKmtQueueRingDoorbell(queue_id_));
   } else {
     // Hardware doorbell supports AQL semantics.
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
     *(signal_.hardware_doorbell_ptr) = uint64_t(value);
     /* signal_ is allocated as uncached so we do not need read-back to flush WC */
   }
@@ -1555,7 +1561,13 @@
   memcpy(&queue_slot[1], &slot_data[1], slot_size_b - sizeof(uint32_t));
   if (IsDeviceMemRingBuf() && needsPcieOrdering()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
   }
   atomic::Store(&queue_slot[0], slot_data[0], std::memory_order_release);
 
diff -urN b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp
--- b/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/runtime/amd_blit_kernel.cpp	2025-11-23 18:03:41.939804910 +0800
@@ -901,7 +901,13 @@
   std::atomic_thread_fence(std::memory_order_release);
   if (queue_->IsDeviceMemRingBuf() && queue_->needsPcieOrdering()) {
     // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
     _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
   }
   __atomic_store_n(&(queue_buffer[index & queue_bitmask_].full_header),
                     kDispatchPacketHeader | packet.setup << 16, __ATOMIC_RELEASE);
diff -urN b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp
--- b/runtime/hsa-runtime/core/runtime/intercept_queue.cpp	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/runtime/intercept_queue.cpp	2025-11-23 18:03:41.939989990 +0800
@@ -268,7 +268,13 @@
       ring[barrier & mask].barrier_and.completion_signal = Signal::Convert(async_doorbell_);
       if (wrapped->IsDeviceMemRingBuf() && needsPcieOrdering()) {
         // Ensure the packet body is written as header may get reordered when writing over PCIE
-        _mm_sfence();
+#if defined(_M_X64)
+    _mm_sfence();
+#elif defined(__loongarch_lp64)
+    asm("dbar 0");
+#elif defined(__riscv)
+    asm volatile("fence w, w");
+#endif
       }
       atomic::Store(&ring[barrier & mask].barrier_and.header, kBarrierHeader,
                     std::memory_order_release);
@@ -315,7 +321,13 @@
       if (write_index != 0) {
         if (wrapped->IsDeviceMemRingBuf() && needsPcieOrdering()) {
           // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
           _mm_sfence();
+#elif defined(__loongarch_lp64)
+          asm("dbar 0");
+#elif defined(__riscv)
+          asm volatile("fence w, w");
+#endif
         }
         atomic::Store(&ring[write & mask].packet.header, packets[first_written_packet_index].packet.header,
                       std::memory_order_release);
@@ -402,7 +414,13 @@
                                                 handler.second, PacketWriter);
     if (IsDeviceMemRingBuf() && needsPcieOrdering()) {
       // Ensure the packet body is written as header may get reordered when writing over PCIE
+#if defined(_M_X64)
       _mm_sfence();
+#elif defined(__loongarch_lp64)
+      asm("dbar 0");
+#elif defined(__riscv)
+      asm volatile("fence w, w");
+#endif
     }
   }
   i = next_packet_;
diff -urN b/runtime/hsa-runtime/core/util/lnx/os_linux.cpp a/runtime/hsa-runtime/core/util/lnx/os_linux.cpp
--- b/runtime/hsa-runtime/core/util/lnx/os_linux.cpp	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/util/lnx/os_linux.cpp	2025-11-23 18:03:41.940183850 +0800
@@ -65,7 +65,7 @@
 #include <cpuid.h>
 #endif
 
-#ifdef __GLIBC__
+#if defined(__GLIBC__) && !defined(__riscv)
 #define ABS_ADDR(base, ptr) (ptr)
 #else
 #define ABS_ADDR(base, ptr) ((base) + (ptr))
diff -urN b/runtime/hsa-runtime/core/util/locks.h a/runtime/hsa-runtime/core/util/locks.h
--- b/runtime/hsa-runtime/core/util/locks.h	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/util/locks.h	2025-11-23 18:03:41.940310290 +0800
@@ -72,7 +72,11 @@
     while (!lock_.compare_exchange_strong(old, 1)) {
       cnt--;
       if (cnt > maxSpinIterPause) {
+#if defined(_M_X64)
         _mm_pause();
+#else
+        os::YieldThread();
+#endif
       } else if (cnt-- > maxSpinIterYield) {
         os::YieldThread();
       } else {
diff -urN b/runtime/hsa-runtime/core/util/utils.h a/runtime/hsa-runtime/core/util/utils.h
--- b/runtime/hsa-runtime/core/util/utils.h	2025-10-24 06:15:14.000000000 +0800
+++ a/runtime/hsa-runtime/core/util/utils.h	2025-11-23 18:03:41.940405560 +0800
@@ -366,6 +366,7 @@
 /// @param: offset(Input), offset of base address to flush
 /// @param: len(Input), length of buffer to flush
 inline void FlushCpuCache(const void* base, size_t offset, size_t len) {
+#if defined(_M_X64)
   static long cacheline_size = 0;
 
   if (!cacheline_size) {
@@ -385,6 +386,11 @@
     _mm_clflush((const void*)cur);
     cur += cacheline_size;
   } while (cur <= (const char*)lastline);
+#elif defined(__riscv)
+  asm volatile("fence rw, rw");
+#elif defined(__loongarch__)
+  asm volatile("nop" ::: "memory");
+#endif
 }
 
 }  // namespace rocr
